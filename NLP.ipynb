{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from pathlib import Path\n",
    "# 下载中文 文本相似度数据集\n",
    "path = Path(kagglehub.dataset_download(\"terrychanorg/lcqmcdata\"))\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(path / \"train.txt\", sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"score\"])\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "#先决定使用的预训练模型\n",
    "# 这个模型是中文比较不错的模型，可以用来做中文文本分类\n",
    "model_nm = \"chinese-roberta-wwm-ext\"\n",
    "local_path = Path(f\"../model/{model_nm}\")\n",
    "local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# 对于NLP模型来说，不同词汇要变成唯一的数字，这个过程叫做tokenization,然后得到了一个映射表，叫做tokenizer\n",
    "# 每个模型都是不一样的映射表，所以要用对应的tokenizer，这里用的是AutoTokenizer，可以自动选择对应的tokenizer\n",
    "tokz  = AutoTokenizer.from_pretrained(local_path,local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize 方法会将输入文本分解为模型词汇表中的 token（子词单元），并返回一个 token 列表。\n",
    "tokz.tokenize(\"我爱吃饭！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,DatasetDict\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "def tok_func(x) : return tokz(x['text1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = ds.map(tok_func, batched=True)\n",
    "tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = tok_ds[0]\n",
    "row['text1'], row['input_ids'], row['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找某个字的映射\n",
    "tokz.vocab['爱']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = tok_ds.rename_columns({'score':'labels'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dds = tok_ds.train_test_split(test_size=0.25,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds,dds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(path / \"test.txt\", sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"score\"])\n",
    "eval_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这里引入使用一下，deeplearning.ipynb里的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal,seed,uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "def noise(x, scale): return normal(scale=scale, size=x.shape)\n",
    "def add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n",
    "def f(x): return 3*x**2+ 4*x+ 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, num=20)[:,None]\n",
    "y = add_noise(f(x), 0.2, 1.3)\n",
    "plt.scatter(x,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(f, min=-2.1, max=2.1, color='r'):\n",
    "    x = np.linspace(min,max, 100)[:,None]\n",
    "    plt.plot(x, f(x), color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\"\"\"\n",
    "plot_poly 用指定阶数（degree）的多项式回归拟合数据 x 和 y。\n",
    "绘制原始数据的散点图（x, y）。\n",
    "绘制模型预测的多项式曲线（通过 plot_function）。\n",
    "\"\"\"\n",
    "\n",
    "def plot_poly(degree):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression()) #\n",
    "    model.fit(x,y)\n",
    "    plt.scatter(x,y)\n",
    "    plot_function(model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_poly(2)\n",
    "plot_function(f,color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_california_housing\n",
    "# housing = fetch_california_housing(as_frame=True)   破玩意，总403  手动下载到本地来用\n",
    "# !wget https://ndownloader.figshare.com/files/5976036 --timeout 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_csv(Path( \"../database/CaliforniaHousing/california_housing_train.csv\"))\n",
    "# 重命名列以匹配 fetch_california_housing\n",
    "housing = housing.rename(columns={\n",
    "    'median_income': 'MedInc',\n",
    "    'housing_median_age': 'HouseAge',\n",
    "    'total_rooms': 'AveRooms',\n",
    "    'total_bedrooms': 'AveBedrms',\n",
    "    'population': 'Population',\n",
    "    'households': 'AveOccup',\n",
    "    'latitude': 'Latitude',\n",
    "    'longitude': 'Longitude',\n",
    "    'median_house_value': 'MedHouseVal'\n",
    "})\n",
    "# 数据计算成平均值\n",
    "housing['AveRooms'] = housing['AveRooms'] / housing['AveOccup']  # total_rooms / households\n",
    "housing['AveBedrms'] = housing['AveBedrms'] / housing['AveOccup'] # total_bedrooms / households\n",
    "housing['AveOccup'] = housing['Population'] / housing['AveOccup'] # population / households\n",
    "# 分离 data 和 target\n",
    "data = housing.drop(columns=['MedHouseVal'])\n",
    "target = housing['MedHouseVal']\n",
    "# 合并并抽样\n",
    "housing = data.join(target).sample(1000, random_state=52)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 seaborn 绘制散点图矩阵\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "# 这个是计算相关系数，相关系数是一个介于-1和1之间的值，-1表示完全负相关，1表示完全正相关，0表示没有相关性\n",
    "# 通俗来说，就是展示两个变量之间关系，是一起变大，还是一个变大另一个就变小\n",
    "np.corrcoef(housing, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(housing.MedInc,housing.MedHouseVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(x,y):return np.corrcoef(x,y)[0][1]\n",
    "\n",
    "corr(housing.MedInc,housing.MedHouseVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_corr(df,a,b):\n",
    "    x,y = df[a],df[b]\n",
    "    plt.scatter(x,y,alpha=0.5,s=4)\n",
    "    plt.title(f'{a} vs {b} ;r:{corr(x,y):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_corr(housing,'MedInc','MedHouseVal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_corr(housing,'MedInc','AveRooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练 相关性识别模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入各种依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from pathlib import Path\n",
    "from transformers import TrainingArguments,DataCollatorWithPadding,Trainer, EarlyStoppingCallback,AutoTokenizer,AutoModelForSequenceClassification,AutoConfig,pipeline\n",
    "import pandas as pd\n",
    "from datasets import Dataset,load_dataset\n",
    "import torch.nn.functional as F\n",
    "from multiprocessing import Pool\n",
    "import kagglehub\n",
    "\n",
    "# # 设置多进程启动方法为 'spawn'\n",
    "# import multiprocessing as mp\n",
    "# mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import time\n",
    "\n",
    "\n",
    "# 加载翻译模型\n",
    "zh_to_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\")\n",
    "en_to_zh = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\")\n",
    "# 加载分词器和模型\n",
    "# tokenizer_zh_en = MarianTokenizer.from_pretrained(model_name_zh_en)\n",
    "# model_zh_en = MarianMTModel.from_pretrained(model_name_zh_en)\n",
    "# tokenizer_en_zh = MarianTokenizer.from_pretrained(model_name_en_zh)\n",
    "# model_en_zh = MarianMTModel.from_pretrained(model_name_en_zh)\n",
    "\n",
    "# # 保存到本地目录\n",
    "# tokenizer_zh_en.save_pretrained(\"./models/opus-mt-zh-en\")\n",
    "# model_zh_en.save_pretrained(\"./models/opus-mt-zh-en\")\n",
    "# tokenizer_en_zh.save_pretrained(\"./models/opus-mt-en-zh\")\n",
    "# model_en_zh.save_pretrained(\"./models/opus-mt-en-zh\")\n",
    "# 测试翻译\n",
    "start = time.time()\n",
    "test_sentence = \"这是一个测试句子。\"\n",
    "en_result = zh_to_en(test_sentence)[0]['translation_text']\n",
    "print(f\"中文 -> 英语: {en_result}\")\n",
    "zh_result = en_to_zh(en_result)[0]['translation_text']\n",
    "print(f\"英语 -> 中文: {zh_result}\")\n",
    "print(f\"单次回译耗时: {time.time() - start} 秒\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预先配置各种数据和东西"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载翻译模型（假设已下载本地模型）\n",
    "zh_to_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\", device=0)  # device=0 使用 GPU\n",
    "en_to_zh = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\", device=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(kagglehub.dataset_download(\"terrychanorg/lcqmcdata\"))\n",
    "\n",
    "# # 加载翻译模型（使用 GPU，假设已下载本地模型）\n",
    "# zh_to_en = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-zh-en\", device=0)\n",
    "# en_to_zh = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-zh\", device=0)\n",
    "\n",
    "# # 定义回译函数\n",
    "# def back_translate(sentence):\n",
    "#     en_sentence = zh_to_en(sentence)[0]['translation_text']\n",
    "#     zh_sentence = en_to_zh(en_sentence)[0]['translation_text']\n",
    "#     return zh_sentence\n",
    "\n",
    "# 读取数据\n",
    "df_tr = pd.read_csv(path / \"train.txt\", sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"labels\"])\n",
    "df_ts = pd.read_csv(path / \"test.txt\", sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"labels\"])\n",
    "\n",
    "# # 只取前 10,000 行进行增强\n",
    "# df_tr_subset = df_tr[:10000]\n",
    "\n",
    "# # 对子集进行回译（单线程）\n",
    "# df_tr_subset['text1_aug'] = df_tr_subset['text1'].apply(back_translate)\n",
    "# df_tr_subset['text2_aug'] = df_tr_subset['text2'].apply(back_translate)\n",
    "\n",
    "# # 创建增强样本\n",
    "# augmented_pairs = pd.concat([\n",
    "#     pd.DataFrame({'text1': df_tr_subset['text1_aug'], 'text2': df_tr_subset['text2'], 'labels': df_tr_subset['labels']}),\n",
    "#     pd.DataFrame({'text1': df_tr_subset['text1'], 'text2': df_tr_subset['text2_aug'], 'labels': df_tr_subset['labels']})\n",
    "# ])\n",
    "\n",
    "# # 合并原始数据和增强数据\n",
    "# df_tr = pd.concat([df_tr, augmented_pairs], ignore_index=True)\n",
    "\n",
    "# # 保存增强后的数据\n",
    "# df_tr.to_csv(\"df_tr_partial_augmented.csv\", index=False, encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_tr = pd.read_csv(\"df_tr_partial_augmented.csv\", encoding='utf-8')\n",
    "text_max = 64\n",
    "# 定义tokenize方法\n",
    "def tok_func(x):\n",
    "    return tokz(x['text1'], x['text2'], padding=True, truncation=True, max_length=text_max)\n",
    "\n",
    "df_tr_clean = df_tr[(df_tr['text1'].str.len() <= text_max) & (df_tr['text2'].str.len() <= text_max)]\n",
    "df_ts_clean = df_ts[(df_ts['text1'].str.len() <= text_max) & (df_ts['text2'].str.len() <= text_max)]\n",
    "\n",
    "ds_tr = Dataset.from_pandas(df_tr_clean)\n",
    "ds_ts = Dataset.from_pandas(df_ts_clean)\n",
    "\n",
    "# 对数据集进行tokenize\n",
    "tok_ds_tr = ds_tr.map(tok_func, batched=True)\n",
    "tok_ds_ts = ds_ts.map(tok_func, batched=True)\n",
    "# # 重命名列\n",
    "# tok_ds_tr = tok_ds_tr.rename_columns({'score':'labels'})\n",
    "# tok_ds_ts = tok_ds_ts.rename_columns({'score':'labels'})\n",
    "\n",
    "# 划分数据集\n",
    "\n",
    "# dds = tok_ds.train_test_split(test_size=0.25,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择使用的模型\n",
    "model_nm = \"chinese-roberta-wwm-ext\"\n",
    "# 选择本地模型路径\n",
    "local_path = Path(f\"../model/{model_nm}\")\n",
    "# 读取模型的tokenizer\n",
    "tokz  = AutoTokenizer.from_pretrained(local_path,local_files_only=True)\n",
    "text_max = 64\n",
    "# 定义tokenize方法\n",
    "def tok_func(x):\n",
    "    return tokz(x['text1'], x['text2'], padding=True, truncation=True, max_length=text_max)\n",
    "\n",
    "df_tr_clean = df_tr[(df_tr['text1'].str.len() <= text_max) & (df_tr['text2'].str.len() <= text_max)]\n",
    "df_ts_clean = df_ts[(df_ts['text1'].str.len() <= text_max) & (df_ts['text2'].str.len() <= text_max)]\n",
    "\n",
    "ds_tr = Dataset.from_pandas(df_tr_clean)\n",
    "ds_ts = Dataset.from_pandas(df_ts_clean)\n",
    "\n",
    "# 对数据集进行tokenize\n",
    "tok_ds_tr = ds_tr.map(tok_func, batched=True)\n",
    "tok_ds_ts = ds_ts.map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds_tr[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 PAWS-X 中文数据集  这个更适合二元分类\n",
    "dataset = load_dataset(\"paws-x\", \"zh\")\n",
    "def rename_columns(example):\n",
    "    return {\n",
    "        \"text1\": example[\"sentence1\"],\n",
    "        \"text2\": example[\"sentence2\"],\n",
    "        \"labels\": example[\"label\"]\n",
    "    }\n",
    "# 分割数据集c\n",
    "train_ds = dataset['train'].rename_columns({\"sentence1\": \"text1\", \"sentence2\": \"text2\", \"label\": \"labels\"})\n",
    "valid_ds = dataset['validation'].rename_columns({\"sentence1\": \"text1\", \"sentence2\": \"text2\", \"label\": \"labels\"})\n",
    "test_ds = dataset['test'].rename_columns({\"sentence1\": \"text1\", \"sentence2\": \"text2\", \"label\": \"labels\"})\n",
    "text_max = 64\n",
    "def tok_func(x):\n",
    "    return tokz(x['text1'], x['text2'], padding=True, truncation=True, max_length=text_max)\n",
    "train_df = train_ds.to_pandas()\n",
    "valid_df = train_ds.to_pandas()\n",
    "# test_df = train_ds.to_pandas()\n",
    "train_df_clean = train_df[(train_df['text1'].str.len() <= text_max) & (train_df['text2'].str.len() <= text_max)]\n",
    "valid_df_clean = valid_df[(valid_df['text1'].str.len() <= text_max) & (valid_df['text2'].str.len() <= text_max)]\n",
    "train_ds = Dataset.from_pandas(train_df_clean)\n",
    "valid_ds = Dataset.from_pandas(valid_df_clean)\n",
    "# 对数据集进行 Tokenize\n",
    "tok_train_ds = train_ds.map(tok_func, batched=True)\n",
    "tok_valid_ds = valid_ds.map(tok_func, batched=True)\n",
    "# tok_test_ds = test_ds.map(tok_func, batched=True)\n",
    "tok_train_ds[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_train_ds.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "print(Counter(tok_train_ds['labels']))\n",
    "print(Counter(tok_valid_ds['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_train_df = tok_train_ds.to_pandas()\n",
    "tok_train_df['len1'] = tok_train_df['text1'].str.len()\n",
    "tok_train_df['len2'] = tok_train_df['text2'].str.len()\n",
    "print(tok_train_df[['len1', 'len2']].describe())  # 查看平均长度和最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 256 # 预设的batch size，可以根据显存大小调整，作用是每次训练多少数据\n",
    "epochs = 3# 训练的轮数\n",
    "lr = 8e-5 # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments('NLP1',learning_rate=lr,warmup_ratio=0.1,lr_scheduler_type='linear', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.3, report_to='none',load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokz)\n",
    "config = AutoConfig.from_pretrained(local_path, num_labels=2, label_smoothing_factor=0.2)\n",
    "# 加载模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    local_path,\n",
    "    config=config\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tok_ds_tr,\n",
    "    eval_dataset=tok_ds_ts,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 上面的模型 微调效果太差，换个试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import os\n",
    "\n",
    "# 创建保存目录\n",
    "save_path = './models/simcse-chinese-roberta-wwm-ext'\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# 下载模型\n",
    "model = SentenceTransformer('cyclone/simcse-chinese-roberta-wwm-ext')\n",
    "\n",
    "# 保存到本地\n",
    "model.save(save_path)\n",
    "print(f\"模型已保存至 {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n",
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers, torch\n",
    "# print(streamlit.__version__)\n",
    "print(sentence_transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(kagglehub.dataset_download(\"terrychanorg/lcqmcdata\"))\n",
    "df_tr = pd.read_csv(str(path / \"train.txt\"), sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"labels\"])\n",
    "df_ts = pd.read_csv(str(path / \"test.txt\"), sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"labels\"])\n",
    "\n",
    "# 过滤数据\n",
    "text_max = 64\n",
    "df_tr_clean = df_tr[(df_tr['text1'].str.len() <= text_max) & (df_tr['text2'].str.len() <= text_max)]\n",
    "df_ts_clean = df_ts[(df_ts['text1'].str.len() <= text_max) & (df_ts['text2'].str.len() <= text_max)]\n",
    "\n",
    "# 准备训练数据\n",
    "train_examples = [InputExample(texts=[row['text1'], row['text2']], label=float(row['labels'])) for _, row in df_tr_clean.iterrows()]\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentence_transformers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43msentence_transformers\u001b[49m\u001b[38;5;241m.\u001b[39m__version__)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentence_transformers' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a55fc0b6b154807a7b08356cd39f8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11193' max='11193' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11193/11193 16:21, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.012400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.010100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.009600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.009100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.008900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.008400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.007600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.007000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.006900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.006800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.006700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.005800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>0.005700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.005500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.005600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 加载本地模型\n",
    "model_nm = \"simcse-chinese-roberta-wwm-ext\"\n",
    "local_path = Path(f\"../model/{model_nm}\")\n",
    "model = SentenceTransformer(str(local_path))\n",
    "# 定义 Contrastive Loss\n",
    "train_loss = losses.ContrastiveLoss(model=model)\n",
    "\n",
    "# 训练\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,\n",
    "    warmup_steps=100,\n",
    "    use_amp=True,  # FP16 适配 4070 Ti Super\n",
    "    output_path='./simcse_finetuned'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试成果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 获取模型和 tokenizer\n",
    "model = trainer.model\n",
    "# tokz = trainer.processing_class  # 新接口，替代 trainer.tokenizer\n",
    "\n",
    "# 确保模型在 GPU 上（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def test_similarity(text1, text2):\n",
    "    # 编码输入\n",
    "    inputs = tokz(text1, text2, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # 推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # 转为概率和预测标签\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "    pred_label = logits.argmax(-1).item()\n",
    "\n",
    "    # 输出结果\n",
    "    print(f\"Text1: {text1}\")\n",
    "    print(f\"Text2: {text2}\")\n",
    "    print(f\"Predicted Label: {pred_label} ({'相似' if pred_label == 1 else '不相似'})\")\n",
    "    print(f\"Similarity Probability: {probs[1]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# 交互式测试\n",
    "while True:\n",
    "    text1 = input(\"请输入第一句话（输入 'exit' 退出）：\")\n",
    "    if text1.lower() == 'exit':\n",
    "        break\n",
    "    text2 = input(\"请输入第二句话：\")\n",
    "    test_similarity(text1, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import streamlit\n",
    "import transformers\n",
    "import torch\n",
    "# print(streamlit.__version__)\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型和 tokenizer\n",
    "output_dir = \"./my_trained_model\"\n",
    "trainer.save_model(output_dir)  # 保存模型\n",
    "tokz.save_pretrained(output_dir)  # 保存 tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
