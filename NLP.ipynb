{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from pathlib import Path\n",
    "# ä¸‹è½½ä¸­æ–‡ æ–‡æœ¬ç›¸ä¼¼åº¦æ•°æ®é›†\n",
    "path = Path(kagglehub.dataset_download(\"terrychanorg/lcqmcdata\"))\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(path / \"train.txt\", sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"score\"])\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "#å…ˆå†³å®šä½¿ç”¨çš„é¢„è®­ç»ƒæ¨¡å‹\n",
    "# è¿™ä¸ªæ¨¡å‹æ˜¯ä¸­æ–‡æ¯”è¾ƒä¸é”™çš„æ¨¡å‹ï¼Œå¯ä»¥ç”¨æ¥åšä¸­æ–‡æ–‡æœ¬åˆ†ç±»\n",
    "model_nm = \"chinese-roberta-wwm-ext\"\n",
    "local_path = Path(f\"../model/{model_nm}\")\n",
    "local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# å¯¹äºNLPæ¨¡å‹æ¥è¯´ï¼Œä¸åŒè¯æ±‡è¦å˜æˆå”¯ä¸€çš„æ•°å­—ï¼Œè¿™ä¸ªè¿‡ç¨‹å«åštokenization,ç„¶åå¾—åˆ°äº†ä¸€ä¸ªæ˜ å°„è¡¨ï¼Œå«åštokenizer\n",
    "# æ¯ä¸ªæ¨¡å‹éƒ½æ˜¯ä¸ä¸€æ ·çš„æ˜ å°„è¡¨ï¼Œæ‰€ä»¥è¦ç”¨å¯¹åº”çš„tokenizerï¼Œè¿™é‡Œç”¨çš„æ˜¯AutoTokenizerï¼Œå¯ä»¥è‡ªåŠ¨é€‰æ‹©å¯¹åº”çš„tokenizer\n",
    "tokz  = AutoTokenizer.from_pretrained(local_path,local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize æ–¹æ³•ä¼šå°†è¾“å…¥æ–‡æœ¬åˆ†è§£ä¸ºæ¨¡å‹è¯æ±‡è¡¨ä¸­çš„ tokenï¼ˆå­è¯å•å…ƒï¼‰ï¼Œå¹¶è¿”å›ä¸€ä¸ª token åˆ—è¡¨ã€‚\n",
    "tokz.tokenize(\"æˆ‘çˆ±åƒé¥­ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,DatasetDict\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "def tok_func(x) : return tokz(x['text1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c7620d011343a480779571ee7eef71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/238766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text1', 'text2', 'score', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 238766\n",
       "})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ds = ds.map(tok_func, batched=True)\n",
    "tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = tok_ds[0]\n",
    "row['text1'], row['input_ids'], row['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŸ¥æ‰¾æŸä¸ªå­—çš„æ˜ å°„\n",
    "tokz.vocab['çˆ±']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = tok_ds.rename_columns({'score':'labels'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b5d48772864162812adeeb2d0c542b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/179074 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf10e4f68ec4978b970aa2bf18eac75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dds = tok_ds.train_test_split(test_size=0.25,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text1', 'text2', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 238766\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text1', 'text2', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "         num_rows: 179074\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text1', 'text2', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "         num_rows: 59692\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ds,dds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(path / \"test.txt\", sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"score\"])\n",
    "eval_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è¿™é‡Œå¼•å…¥ä½¿ç”¨ä¸€ä¸‹ï¼Œdeeplearning.ipynbé‡Œçš„ä»£ç "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal,seed,uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "def noise(x, scale): return normal(scale=scale, size=x.shape)\n",
    "def add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n",
    "def f(x): return 3*x**2+ 4*x+ 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, num=20)[:,None]\n",
    "y = add_noise(f(x), 0.2, 1.3)\n",
    "plt.scatter(x,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(f, min=-2.1, max=2.1, color='r'):\n",
    "    x = np.linspace(min,max, 100)[:,None]\n",
    "    plt.plot(x, f(x), color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\"\"\"\n",
    "plot_poly ç”¨æŒ‡å®šé˜¶æ•°ï¼ˆdegreeï¼‰çš„å¤šé¡¹å¼å›å½’æ‹Ÿåˆæ•°æ® x å’Œ yã€‚\n",
    "ç»˜åˆ¶åŸå§‹æ•°æ®çš„æ•£ç‚¹å›¾ï¼ˆx, yï¼‰ã€‚\n",
    "ç»˜åˆ¶æ¨¡å‹é¢„æµ‹çš„å¤šé¡¹å¼æ›²çº¿ï¼ˆé€šè¿‡ plot_functionï¼‰ã€‚\n",
    "\"\"\"\n",
    "\n",
    "def plot_poly(degree):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression()) #\n",
    "    model.fit(x,y)\n",
    "    plt.scatter(x,y)\n",
    "    plot_function(model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_poly(2)\n",
    "plot_function(f,color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ•°æ®çš„ç›¸å…³æ€§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_california_housing\n",
    "# housing = fetch_california_housing(as_frame=True)   ç ´ç©æ„ï¼Œæ€»403  æ‰‹åŠ¨ä¸‹è½½åˆ°æœ¬åœ°æ¥ç”¨\n",
    "# !wget https://ndownloader.figshare.com/files/5976036 --timeout 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_csv(Path( \"../database/CaliforniaHousing/california_housing_train.csv\"))\n",
    "# é‡å‘½ååˆ—ä»¥åŒ¹é… fetch_california_housing\n",
    "housing = housing.rename(columns={\n",
    "    'median_income': 'MedInc',\n",
    "    'housing_median_age': 'HouseAge',\n",
    "    'total_rooms': 'AveRooms',\n",
    "    'total_bedrooms': 'AveBedrms',\n",
    "    'population': 'Population',\n",
    "    'households': 'AveOccup',\n",
    "    'latitude': 'Latitude',\n",
    "    'longitude': 'Longitude',\n",
    "    'median_house_value': 'MedHouseVal'\n",
    "})\n",
    "# æ•°æ®è®¡ç®—æˆå¹³å‡å€¼\n",
    "housing['AveRooms'] = housing['AveRooms'] / housing['AveOccup']  # total_rooms / households\n",
    "housing['AveBedrms'] = housing['AveBedrms'] / housing['AveOccup'] # total_bedrooms / households\n",
    "housing['AveOccup'] = housing['Population'] / housing['AveOccup'] # population / households\n",
    "# åˆ†ç¦» data å’Œ target\n",
    "data = housing.drop(columns=['MedHouseVal'])\n",
    "target = housing['MedHouseVal']\n",
    "# åˆå¹¶å¹¶æŠ½æ ·\n",
    "housing = data.join(target).sample(1000, random_state=52)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”¨ seaborn ç»˜åˆ¶æ•£ç‚¹å›¾çŸ©é˜µ\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "# è¿™ä¸ªæ˜¯è®¡ç®—ç›¸å…³ç³»æ•°ï¼Œç›¸å…³ç³»æ•°æ˜¯ä¸€ä¸ªä»‹äº-1å’Œ1ä¹‹é—´çš„å€¼ï¼Œ-1è¡¨ç¤ºå®Œå…¨è´Ÿç›¸å…³ï¼Œ1è¡¨ç¤ºå®Œå…¨æ­£ç›¸å…³ï¼Œ0è¡¨ç¤ºæ²¡æœ‰ç›¸å…³æ€§\n",
    "# é€šä¿—æ¥è¯´ï¼Œå°±æ˜¯å±•ç¤ºä¸¤ä¸ªå˜é‡ä¹‹é—´å…³ç³»ï¼Œæ˜¯ä¸€èµ·å˜å¤§ï¼Œè¿˜æ˜¯ä¸€ä¸ªå˜å¤§å¦ä¸€ä¸ªå°±å˜å°\n",
    "np.corrcoef(housing, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(housing.MedInc,housing.MedHouseVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(x,y):return np.corrcoef(x,y)[0][1]\n",
    "\n",
    "corr(housing.MedInc,housing.MedHouseVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_corr(df,a,b):\n",
    "    x,y = df[a],df[b]\n",
    "    plt.scatter(x,y,alpha=0.5,s=4)\n",
    "    plt.title(f'{a} vs {b} ;r:{corr(x,y):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_corr(housing,'MedInc','MedHouseVal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_corr(housing,'MedInc','AveRooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¼€å§‹è®­ç»ƒ ç›¸å…³æ€§è¯†åˆ«æ¨¡å‹"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¼•å…¥å„ç§ä¾èµ–"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from pathlib import Path\n",
    "from transformers import TrainingArguments,DataCollatorWithPadding,Trainer, EarlyStoppingCallback,AutoTokenizer,AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from datasets import Dataset,concatenate_datasets\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é¢„å…ˆé…ç½®å„ç§æ•°æ®å’Œä¸œè¥¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ•°æ®é›†\n",
    "# ä¸‹è½½ä¸­æ–‡ æ–‡æœ¬ç›¸ä¼¼åº¦æ•°æ®é›†\n",
    "path = Path(kagglehub.dataset_download(\"terrychanorg/lcqmcdata\"))\n",
    "\n",
    "# è¯»å–æ•°æ®\n",
    "df_tr = pd.read_csv(path / \"train.txt\", sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"labels\"])\n",
    "df_ts = pd.read_csv(path / \"test.txt\", sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e8f465762e46868201fb47662a2afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/238766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3959a4d7e5a494a862a91cf7993d574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# é€‰æ‹©ä½¿ç”¨çš„æ¨¡å‹\n",
    "model_nm = \"chinese-roberta-wwm-ext\"\n",
    "# é€‰æ‹©æœ¬åœ°æ¨¡å‹è·¯å¾„\n",
    "local_path = Path(f\"../model/{model_nm}\")\n",
    "# è¯»å–æ¨¡å‹çš„tokenizer\n",
    "tokz  = AutoTokenizer.from_pretrained(local_path,local_files_only=True)\n",
    "# æŠŠæ•°æ®è½¬æ¢æˆdataset\n",
    "ds_tr = Dataset.from_pandas(df_tr)\n",
    "ds_ts = Dataset.from_pandas(df_ts)\n",
    "# å®šä¹‰tokenizeæ–¹æ³•\n",
    "def tok_func(x):\n",
    "    return tokz(x['text1'], x['text2'], padding=True, truncation=True, max_length=48)\n",
    "# å¯¹æ•°æ®é›†è¿›è¡Œtokenize\n",
    "tok_ds_tr = ds_tr.map(tok_func, batched=True)\n",
    "tok_ds_ts = ds_ts.map(tok_func, batched=True)\n",
    "# # é‡å‘½ååˆ—\n",
    "# tok_ds_tr = tok_ds_tr.rename_columns({'score':'labels'})\n",
    "# tok_ds_ts = tok_ds_ts.rename_columns({'score':'labels'})\n",
    "\n",
    "# åˆ’åˆ†æ•°æ®é›†\n",
    "\n",
    "# dds = tok_ds.train_test_split(test_size=0.25,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>labels</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>å–œæ¬¢æ‰“ç¯®çƒçš„ç”·ç”Ÿå–œæ¬¢ä»€ä¹ˆæ ·çš„å¥³ç”Ÿ</td>\n",
       "      <td>çˆ±æ‰“ç¯®çƒçš„ç”·ç”Ÿå–œæ¬¢ä»€ä¹ˆæ ·çš„å¥³ç”Ÿ</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 1599, 3614, 2802, 5074, 4413, 4638, 4511...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>æˆ‘æ‰‹æœºä¸¢äº†ï¼Œæˆ‘æƒ³æ¢ä¸ªæ‰‹æœº</td>\n",
       "      <td>æˆ‘æƒ³ä¹°ä¸ªæ–°æ‰‹æœºï¼Œæ±‚æ¨è</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2769, 2797, 3322, 696, 749, 8024, 2769, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>å¤§å®¶è§‰å¾—å¥¹å¥½çœ‹å—</td>\n",
       "      <td>å¤§å®¶è§‰å¾—è·‘ç”·å¥½çœ‹å—ï¼Ÿ</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1920, 2157, 6230, 2533, 1961, 1962, 4692...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>æ±‚ç§‹è‰²ä¹‹ç©ºæ¼«ç”»å…¨é›†</td>\n",
       "      <td>æ±‚ç§‹è‰²ä¹‹ç©ºå…¨é›†æ¼«ç”»</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 3724, 4904, 5682, 722, 4958, 4035, 4514,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>æ™šä¸Šç¡è§‰å¸¦ç€è€³æœºå¬éŸ³ä¹æœ‰ä»€ä¹ˆå®³å¤„å—ï¼Ÿ</td>\n",
       "      <td>å­•å¦‡å¯ä»¥æˆ´è€³æœºå¬éŸ³ä¹å—?</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 3241, 677, 4717, 6230, 2372, 4708, 5455,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text1            text2  labels  \\\n",
       "0    å–œæ¬¢æ‰“ç¯®çƒçš„ç”·ç”Ÿå–œæ¬¢ä»€ä¹ˆæ ·çš„å¥³ç”Ÿ  çˆ±æ‰“ç¯®çƒçš„ç”·ç”Ÿå–œæ¬¢ä»€ä¹ˆæ ·çš„å¥³ç”Ÿ       1   \n",
       "1        æˆ‘æ‰‹æœºä¸¢äº†ï¼Œæˆ‘æƒ³æ¢ä¸ªæ‰‹æœº      æˆ‘æƒ³ä¹°ä¸ªæ–°æ‰‹æœºï¼Œæ±‚æ¨è       1   \n",
       "2            å¤§å®¶è§‰å¾—å¥¹å¥½çœ‹å—       å¤§å®¶è§‰å¾—è·‘ç”·å¥½çœ‹å—ï¼Ÿ       0   \n",
       "3           æ±‚ç§‹è‰²ä¹‹ç©ºæ¼«ç”»å…¨é›†        æ±‚ç§‹è‰²ä¹‹ç©ºå…¨é›†æ¼«ç”»       1   \n",
       "4  æ™šä¸Šç¡è§‰å¸¦ç€è€³æœºå¬éŸ³ä¹æœ‰ä»€ä¹ˆå®³å¤„å—ï¼Ÿ     å­•å¦‡å¯ä»¥æˆ´è€³æœºå¬éŸ³ä¹å—?       0   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 1599, 3614, 2802, 5074, 4413, 4638, 4511...   \n",
       "1  [101, 2769, 2797, 3322, 696, 749, 8024, 2769, ...   \n",
       "2  [101, 1920, 2157, 6230, 2533, 1961, 1962, 4692...   \n",
       "3  [101, 3724, 4904, 5682, 722, 4958, 4035, 4514,...   \n",
       "4  [101, 3241, 677, 4717, 6230, 2372, 4708, 5455,...   \n",
       "\n",
       "                                      token_type_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ds.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 103786, 0: 75288})\n",
      "Counter({1: 34788, 0: 24904})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(dds['train']['labels']))\n",
    "print(Counter(dds['test']['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                len1           len2\n",
      "count  238766.000000  238766.000000\n",
      "mean       10.668177      11.209586\n",
      "std         4.087534       4.813823\n",
      "min         2.000000       2.000000\n",
      "25%         8.000000       8.000000\n",
      "50%        10.000000      10.000000\n",
      "75%        12.000000      13.000000\n",
      "max        49.000000     131.000000\n"
     ]
    }
   ],
   "source": [
    "df_tr['len1'] = df_tr['text1'].str.len()\n",
    "df_tr['len2'] = df_tr['text2'].str.len()\n",
    "print(df_tr[['len1', 'len2']].describe())  # æŸ¥çœ‹å¹³å‡é•¿åº¦å’Œæœ€å¤§é•¿åº¦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 521 # é¢„è®¾çš„batch sizeï¼Œå¯ä»¥æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´ï¼Œä½œç”¨æ˜¯æ¯æ¬¡è®­ç»ƒå¤šå°‘æ•°æ®\n",
    "epochs = 4 # è®­ç»ƒçš„è½®æ•°\n",
    "lr = 3e-5 # å­¦ä¹ ç‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inksnow/.pyenv/versions/3.10.9/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments('NLP1',learning_rate=lr,warmup_ratio=0.1,lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.25, report_to='none',load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokz)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    local_path,\n",
    "    num_labels=2,\n",
    "    weights_only=False,\n",
    "    use_safetensors=False,\n",
    ")\n",
    "# trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "#                   tokenizer=tokz, compute_metrics=corr_d)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tok_ds_tr,\n",
    "    eval_dataset=tok_ds_ts,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='../model/chinese-roberta-wwm-ext', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æµ‹è¯•æˆæœ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1: åƒé¥­äº†å§\n",
      "Text2: åƒé¥­ä¸äº†ï¼Ÿ\n",
      "Predicted Label: 0 (ä¸ç›¸ä¼¼)\n",
      "Similarity Probability: 0.1242\n",
      "\n",
      "Text1: \n",
      "Text2: æˆ‘çˆ±ä½ \n",
      "Predicted Label: 0 (ä¸ç›¸ä¼¼)\n",
      "Similarity Probability: 0.4211\n",
      "\n",
      "Text1: ä½ çˆ±æˆ‘\n",
      "Text2: \n",
      "Predicted Label: 0 (ä¸ç›¸ä¼¼)\n",
      "Similarity Probability: 0.0153\n",
      "\n",
      "Text1: æˆ‘çˆ±ä½ \n",
      "Text2: ä½ çˆ±æˆ‘\n",
      "Predicted Label: 1 (ç›¸ä¼¼)\n",
      "Similarity Probability: 0.6672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# è·å–æ¨¡å‹å’Œ tokenizer\n",
    "model = trainer.model\n",
    "# tokz = trainer.processing_class  # æ–°æ¥å£ï¼Œæ›¿ä»£ trainer.tokenizer\n",
    "\n",
    "# ç¡®ä¿æ¨¡å‹åœ¨ GPU ä¸Šï¼ˆå¦‚æœå¯ç”¨ï¼‰\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def test_similarity(text1, text2):\n",
    "    # ç¼–ç è¾“å…¥\n",
    "    inputs = tokz(text1, text2, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # æ¨ç†\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # è½¬ä¸ºæ¦‚ç‡å’Œé¢„æµ‹æ ‡ç­¾\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "    pred_label = logits.argmax(-1).item()\n",
    "\n",
    "    # è¾“å‡ºç»“æœ\n",
    "    print(f\"Text1: {text1}\")\n",
    "    print(f\"Text2: {text2}\")\n",
    "    print(f\"Predicted Label: {pred_label} ({'ç›¸ä¼¼' if pred_label == 1 else 'ä¸ç›¸ä¼¼'})\")\n",
    "    print(f\"Similarity Probability: {probs[1]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# äº¤äº’å¼æµ‹è¯•\n",
    "while True:\n",
    "    text1 = input(\"è¯·è¾“å…¥ç¬¬ä¸€å¥è¯ï¼ˆè¾“å…¥ 'exit' é€€å‡ºï¼‰ï¼š\")\n",
    "    if text1.lower() == 'exit':\n",
    "        break\n",
    "    text2 = input(\"è¯·è¾“å…¥ç¬¬äºŒå¥è¯ï¼š\")\n",
    "    test_similarity(text1, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.49.0\n",
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# import streamlit\n",
    "import transformers\n",
    "import torch\n",
    "# print(streamlit.__version__)\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_trained_model/tokenizer_config.json',\n",
       " './my_trained_model/special_tokens_map.json',\n",
       " './my_trained_model/vocab.txt',\n",
       " './my_trained_model/added_tokens.json',\n",
       " './my_trained_model/tokenizer.json')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ä¿å­˜æ¨¡å‹å’Œ tokenizer\n",
    "output_dir = \"./my_trained_model\"\n",
    "trainer.save_model(output_dir)  # ä¿å­˜æ¨¡å‹\n",
    "tokz.save_pretrained(output_dir)  # ä¿å­˜ tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
