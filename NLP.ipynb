{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install transformers\n",
    "# %pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from pathlib import Path\n",
    "# 下载中文 文本相似度数据集\n",
    "path = Path(kagglehub.dataset_download(\"terrychanorg/lcqmcdata\"))\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(path / \"train.txt\", sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"score\"])\n",
    "df.describe(include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "#先决定使用的预训练模型\n",
    "# 这个模型是中文比较不错的模型，可以用来做中文文本分类\n",
    "model_nm = \"chinese-roberta-wwm-ext\"\n",
    "local_path = Path(f\"../model/{model_nm}\")\n",
    "local_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "# 对于NLP模型来说，不同词汇要变成唯一的数字，这个过程叫做tokenization,然后得到了一个映射表，叫做tokenizer\n",
    "# 每个模型都是不一样的映射表，所以要用对应的tokenizer，这里用的是AutoTokenizer，可以自动选择对应的tokenizer\n",
    "tokz  = AutoTokenizer.from_pretrained(local_path,local_files_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize 方法会将输入文本分解为模型词汇表中的 token（子词单元），并返回一个 token 列表。\n",
    "tokz.tokenize(\"我爱吃饭！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset,DatasetDict\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "def tok_func(x) : return tokz(x['text1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c7620d011343a480779571ee7eef71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/238766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text1', 'text2', 'score', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 238766\n",
       "})"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ds = ds.map(tok_func, batched=True)\n",
    "tok_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = tok_ds[0]\n",
    "row['text1'], row['input_ids'], row['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查找某个字的映射\n",
    "tokz.vocab['爱']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = tok_ds.rename_columns({'score':'labels'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8b5d48772864162812adeeb2d0c542b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/179074 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf10e4f68ec4978b970aa2bf18eac75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/59692 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dds = tok_ds.train_test_split(test_size=0.25,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['text1', 'text2', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "     num_rows: 238766\n",
       " }),\n",
       " DatasetDict({\n",
       "     train: Dataset({\n",
       "         features: ['text1', 'text2', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "         num_rows: 179074\n",
       "     })\n",
       "     test: Dataset({\n",
       "         features: ['text1', 'text2', 'labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "         num_rows: 59692\n",
       "     })\n",
       " }))"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ds,dds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.read_csv(path / \"test.txt\", sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"score\"])\n",
    "eval_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这里引入使用一下，deeplearning.ipynb里的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal,seed,uniform\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "def noise(x, scale): return normal(scale=scale, size=x.shape)\n",
    "def add_noise(x, mult, add): return x * (1+noise(x,mult)) + noise(x,add)\n",
    "def f(x): return 3*x**2+ 4*x+ 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2, 2, num=20)[:,None]\n",
    "y = add_noise(f(x), 0.2, 1.3)\n",
    "plt.scatter(x,y);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(f, min=-2.1, max=2.1, color='r'):\n",
    "    x = np.linspace(min,max, 100)[:,None]\n",
    "    plt.plot(x, f(x), color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\"\"\"\n",
    "plot_poly 用指定阶数（degree）的多项式回归拟合数据 x 和 y。\n",
    "绘制原始数据的散点图（x, y）。\n",
    "绘制模型预测的多项式曲线（通过 plot_function）。\n",
    "\"\"\"\n",
    "\n",
    "def plot_poly(degree):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression()) #\n",
    "    model.fit(x,y)\n",
    "    plt.scatter(x,y)\n",
    "    plot_function(model.predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_poly(2)\n",
    "plot_function(f,color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据的相关性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import fetch_california_housing\n",
    "# housing = fetch_california_housing(as_frame=True)   破玩意，总403  手动下载到本地来用\n",
    "# !wget https://ndownloader.figshare.com/files/5976036 --timeout 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "housing = pd.read_csv(Path( \"../database/CaliforniaHousing/california_housing_train.csv\"))\n",
    "# 重命名列以匹配 fetch_california_housing\n",
    "housing = housing.rename(columns={\n",
    "    'median_income': 'MedInc',\n",
    "    'housing_median_age': 'HouseAge',\n",
    "    'total_rooms': 'AveRooms',\n",
    "    'total_bedrooms': 'AveBedrms',\n",
    "    'population': 'Population',\n",
    "    'households': 'AveOccup',\n",
    "    'latitude': 'Latitude',\n",
    "    'longitude': 'Longitude',\n",
    "    'median_house_value': 'MedHouseVal'\n",
    "})\n",
    "# 数据计算成平均值\n",
    "housing['AveRooms'] = housing['AveRooms'] / housing['AveOccup']  # total_rooms / households\n",
    "housing['AveBedrms'] = housing['AveBedrms'] / housing['AveOccup'] # total_bedrooms / households\n",
    "housing['AveOccup'] = housing['Population'] / housing['AveOccup'] # population / households\n",
    "# 分离 data 和 target\n",
    "data = housing.drop(columns=['MedHouseVal'])\n",
    "target = housing['MedHouseVal']\n",
    "# 合并并抽样\n",
    "housing = data.join(target).sample(1000, random_state=52)\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用 seaborn 绘制散点图矩阵\n",
    "np.set_printoptions(precision=2, suppress=True)\n",
    "# 这个是计算相关系数，相关系数是一个介于-1和1之间的值，-1表示完全负相关，1表示完全正相关，0表示没有相关性\n",
    "# 通俗来说，就是展示两个变量之间关系，是一起变大，还是一个变大另一个就变小\n",
    "np.corrcoef(housing, rowvar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(housing.MedInc,housing.MedHouseVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr(x,y):return np.corrcoef(x,y)[0][1]\n",
    "\n",
    "corr(housing.MedInc,housing.MedHouseVal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_corr(df,a,b):\n",
    "    x,y = df[a],df[b]\n",
    "    plt.scatter(x,y,alpha=0.5,s=4)\n",
    "    plt.title(f'{a} vs {b} ;r:{corr(x,y):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_corr(housing,'MedInc','MedHouseVal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_corr(housing,'MedInc','AveRooms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_d(eval_pred): return {'pearson': corr(*eval_pred)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 开始训练 相关性识别模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 引入各种依赖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "from pathlib import Path\n",
    "from transformers import TrainingArguments,DataCollatorWithPadding,Trainer, EarlyStoppingCallback,AutoTokenizer,AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "from datasets import Dataset,concatenate_datasets\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预先配置各种数据和东西"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集\n",
    "# 下载中文 文本相似度数据集\n",
    "path = Path(kagglehub.dataset_download(\"terrychanorg/lcqmcdata\"))\n",
    "\n",
    "# 读取数据\n",
    "df_tr = pd.read_csv(path / \"train.txt\", sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"labels\"])\n",
    "df_ts = pd.read_csv(path / \"test.txt\", sep=\"\\t\", header=None, names=[\"text1\", \"text2\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00e8f465762e46868201fb47662a2afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/238766 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3959a4d7e5a494a862a91cf7993d574",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# 选择使用的模型\n",
    "model_nm = \"chinese-roberta-wwm-ext\"\n",
    "# 选择本地模型路径\n",
    "local_path = Path(f\"../model/{model_nm}\")\n",
    "# 读取模型的tokenizer\n",
    "tokz  = AutoTokenizer.from_pretrained(local_path,local_files_only=True)\n",
    "# 把数据转换成dataset\n",
    "ds_tr = Dataset.from_pandas(df_tr)\n",
    "ds_ts = Dataset.from_pandas(df_ts)\n",
    "# 定义tokenize方法\n",
    "def tok_func(x):\n",
    "    return tokz(x['text1'], x['text2'], padding=True, truncation=True, max_length=48)\n",
    "# 对数据集进行tokenize\n",
    "tok_ds_tr = ds_tr.map(tok_func, batched=True)\n",
    "tok_ds_ts = ds_ts.map(tok_func, batched=True)\n",
    "# # 重命名列\n",
    "# tok_ds_tr = tok_ds_tr.rename_columns({'score':'labels'})\n",
    "# tok_ds_ts = tok_ds_ts.rename_columns({'score':'labels'})\n",
    "\n",
    "# 划分数据集\n",
    "\n",
    "# dds = tok_ds.train_test_split(test_size=0.25,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>labels</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>喜欢打篮球的男生喜欢什么样的女生</td>\n",
       "      <td>爱打篮球的男生喜欢什么样的女生</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 1599, 3614, 2802, 5074, 4413, 4638, 4511...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>我手机丢了，我想换个手机</td>\n",
       "      <td>我想买个新手机，求推荐</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 2769, 2797, 3322, 696, 749, 8024, 2769, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>大家觉得她好看吗</td>\n",
       "      <td>大家觉得跑男好看吗？</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 1920, 2157, 6230, 2533, 1961, 1962, 4692...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>求秋色之空漫画全集</td>\n",
       "      <td>求秋色之空全集漫画</td>\n",
       "      <td>1</td>\n",
       "      <td>[101, 3724, 4904, 5682, 722, 4958, 4035, 4514,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>晚上睡觉带着耳机听音乐有什么害处吗？</td>\n",
       "      <td>孕妇可以戴耳机听音乐吗?</td>\n",
       "      <td>0</td>\n",
       "      <td>[101, 3241, 677, 4717, 6230, 2372, 4708, 5455,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                text1            text2  labels  \\\n",
       "0    喜欢打篮球的男生喜欢什么样的女生  爱打篮球的男生喜欢什么样的女生       1   \n",
       "1        我手机丢了，我想换个手机      我想买个新手机，求推荐       1   \n",
       "2            大家觉得她好看吗       大家觉得跑男好看吗？       0   \n",
       "3           求秋色之空漫画全集        求秋色之空全集漫画       1   \n",
       "4  晚上睡觉带着耳机听音乐有什么害处吗？     孕妇可以戴耳机听音乐吗?       0   \n",
       "\n",
       "                                           input_ids  \\\n",
       "0  [101, 1599, 3614, 2802, 5074, 4413, 4638, 4511...   \n",
       "1  [101, 2769, 2797, 3322, 696, 749, 8024, 2769, ...   \n",
       "2  [101, 1920, 2157, 6230, 2533, 1961, 1962, 4692...   \n",
       "3  [101, 3724, 4904, 5682, 722, 4958, 4035, 4514,...   \n",
       "4  [101, 3241, 677, 4717, 6230, 2372, 4708, 5455,...   \n",
       "\n",
       "                                      token_type_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                      attention_mask  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ds.to_pandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 103786, 0: 75288})\n",
      "Counter({1: 34788, 0: 24904})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "print(Counter(dds['train']['labels']))\n",
    "print(Counter(dds['test']['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                len1           len2\n",
      "count  238766.000000  238766.000000\n",
      "mean       10.668177      11.209586\n",
      "std         4.087534       4.813823\n",
      "min         2.000000       2.000000\n",
      "25%         8.000000       8.000000\n",
      "50%        10.000000      10.000000\n",
      "75%        12.000000      13.000000\n",
      "max        49.000000     131.000000\n"
     ]
    }
   ],
   "source": [
    "df_tr['len1'] = df_tr['text1'].str.len()\n",
    "df_tr['len2'] = df_tr['text2'].str.len()\n",
    "print(df_tr[['len1', 'len2']].describe())  # 查看平均长度和最大长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 521 # 预设的batch size，可以根据显存大小调整，作用是每次训练多少数据\n",
    "epochs = 4 # 训练的轮数\n",
    "lr = 3e-5 # 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inksnow/.pyenv/versions/3.10.9/lib/python3.10/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments('NLP1',learning_rate=lr,warmup_ratio=0.1,lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=0.25, report_to='none',load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_strategy=\"epoch\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../model/chinese-roberta-wwm-ext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokz)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    local_path,\n",
    "    num_labels=2,\n",
    "    weights_only=False,\n",
    "    use_safetensors=False,\n",
    ")\n",
    "# trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "#                   tokenizer=tokz, compute_metrics=corr_d)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tok_ds_tr,\n",
    "    eval_dataset=tok_ds_ts,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda p: {\"accuracy\": (p.predictions.argmax(-1) == p.label_ids).mean()},\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2, early_stopping_threshold=0.0)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertTokenizerFast(name_or_path='../model/chinese-roberta-wwm-ext', vocab_size=21128, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"[PAD]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t100: AddedToken(\"[UNK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t101: AddedToken(\"[CLS]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t102: AddedToken(\"[SEP]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t103: AddedToken(\"[MASK]\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试成果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text1: 吃饭了吧\n",
      "Text2: 吃饭不了？\n",
      "Predicted Label: 0 (不相似)\n",
      "Similarity Probability: 0.1242\n",
      "\n",
      "Text1: \n",
      "Text2: 我爱你\n",
      "Predicted Label: 0 (不相似)\n",
      "Similarity Probability: 0.4211\n",
      "\n",
      "Text1: 你爱我\n",
      "Text2: \n",
      "Predicted Label: 0 (不相似)\n",
      "Similarity Probability: 0.0153\n",
      "\n",
      "Text1: 我爱你\n",
      "Text2: 你爱我\n",
      "Predicted Label: 1 (相似)\n",
      "Similarity Probability: 0.6672\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 获取模型和 tokenizer\n",
    "model = trainer.model\n",
    "# tokz = trainer.processing_class  # 新接口，替代 trainer.tokenizer\n",
    "\n",
    "# 确保模型在 GPU 上（如果可用）\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "def test_similarity(text1, text2):\n",
    "    # 编码输入\n",
    "    inputs = tokz(text1, text2, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # 推理\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # 转为概率和预测标签\n",
    "    probs = torch.softmax(logits, dim=-1).cpu().numpy()[0]\n",
    "    pred_label = logits.argmax(-1).item()\n",
    "\n",
    "    # 输出结果\n",
    "    print(f\"Text1: {text1}\")\n",
    "    print(f\"Text2: {text2}\")\n",
    "    print(f\"Predicted Label: {pred_label} ({'相似' if pred_label == 1 else '不相似'})\")\n",
    "    print(f\"Similarity Probability: {probs[1]:.4f}\")\n",
    "    print()\n",
    "\n",
    "# 交互式测试\n",
    "while True:\n",
    "    text1 = input(\"请输入第一句话（输入 'exit' 退出）：\")\n",
    "    if text1.lower() == 'exit':\n",
    "        break\n",
    "    text2 = input(\"请输入第二句话：\")\n",
    "    test_similarity(text1, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.49.0\n",
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "# import streamlit\n",
    "import transformers\n",
    "import torch\n",
    "# print(streamlit.__version__)\n",
    "print(transformers.__version__)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_trained_model/tokenizer_config.json',\n",
       " './my_trained_model/special_tokens_map.json',\n",
       " './my_trained_model/vocab.txt',\n",
       " './my_trained_model/added_tokens.json',\n",
       " './my_trained_model/tokenizer.json')"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 保存模型和 tokenizer\n",
    "output_dir = \"./my_trained_model\"\n",
    "trainer.save_model(output_dir)  # 保存模型\n",
    "tokz.save_pretrained(output_dir)  # 保存 tokenizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
